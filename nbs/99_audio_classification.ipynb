{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp audio_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import audioengine\n",
    "from audioengine.utils.gpu import list_all_gpus, set_gpu_list_memory_limit\n",
    "from audioengine.utils.schema import verify_audioengine_dataset, verify_audioengine_internal_audio_representation_schema\n",
    "from audioengine.utils.misc import log_init, log_info, log_debug, log_error, pad_up_to\n",
    "from audioengine.models import Simple1DConvNet\n",
    "from audioengine.utils.wav_utils import get_max_samples_in_wav_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DEBUG == True):\n",
    "    physical_gpus_list = list_all_gpus()\n",
    "    print(physical_gpus_list)\n",
    "    set_gpu_list_memory_limit(physical_gpus_list, limit=2**13)\n",
    "    \n",
    "    log_init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset\n",
    "\n",
    "This notebook will be using the single word dataset which is a subset of the common voice dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIRECTORY = '/project/Datasets/audioengine_single_word'\n",
    "DATASET_NAME = 'dev.json'\n",
    "AUDIO_CLIPS_DIR_NAME = 'clips'\n",
    "\n",
    "DATASET_JSON_FILEPATH = os.path.join(DATASET_DIRECTORY, DATASET_NAME)\n",
    "AUDIO_CLIPS_FULL_DIR_PATH = os.path.join(DATASET_DIRECTORY, AUDIO_CLIPS_DIR_NAME)\n",
    "\n",
    "dataset_json_fp = open(DATASET_JSON_FILEPATH, 'r')\n",
    "dataset_json = json.load(dataset_json_fp)\n",
    "dataset_json_fp.close()\n",
    "\n",
    "if(not verify_audioengine_dataset(dataset_json)):\n",
    "    log_critical('The dataset does not match the schema!')\n",
    "else:\n",
    "    log_debug('The dataset matches the schema')\n",
    "    \n",
    "def create_ir_json(partial_json: dict, audio_clip_directory: str, length_to_pad_to: int) -> dict:\n",
    "    audio_data_id = partial_json['id']\n",
    "    file_name = partial_json['file_name']\n",
    "    full_audio_clip_filepath = os.path.join(audio_clip_directory, file_name)\n",
    "    contents = tf.io.read_file(full_audio_clip_filepath)\n",
    "    audio_data, _ = tf.audio.decode_wav(contents)\n",
    "    audio_data = tf.squeeze(audio_data, axis=1)\n",
    "    audio_data = pad_up_to(audio_data, (length_to_pad_to,), 0)\n",
    "    audio_data = tf.expand_dims(audio_data, axis=-1)\n",
    "\n",
    "\n",
    "    ir_record_json = {'audio_data': audio_data,\n",
    "                      'length': tf.shape(audio_data)[0],\n",
    "                      'id': audio_data_id,\n",
    "                      'file_name': file_name,\n",
    "                      'category_id': partial_json['category_id']\n",
    "                     }\n",
    "    return ir_record_json\n",
    "\n",
    "def convert_classification_audioengine_dataset_to_IR_generator(dataset_json: dict, audio_clip_directory: str,\n",
    "                                                               batch_size: int = 256) -> list:\n",
    "    '''\n",
    "    This uses too much memory to hold the whole dataset at once\n",
    "    Need to use generators instead.\n",
    "    '''\n",
    "    #Really needs multiprocessing in the future\n",
    "    if(not dataset_json['info']['task'] == 'classification'):\n",
    "        log_critical('Dataset not using classification task')\n",
    "    else:\n",
    "        log_debug('Dataset using classification task')\n",
    "    \n",
    "    audio_dataset_section_json = dataset_json['audio']\n",
    "    \n",
    "    #Batch it here\n",
    "    num_batches = math.floor((len(audio_dataset_section_json) - (len(audio_dataset_section_json) % batch_size)) / batch_size)\n",
    "    left_over = len(audio_dataset_section_json) % batch_size\n",
    "    \n",
    "    #ir_list = []\n",
    "    max_length = get_max_samples_in_wav_from_directory(audio_clip_directory)\n",
    "    for i in range(num_batches):\n",
    "        batch_ir_list = []\n",
    "        batch_features_list = []\n",
    "        batch_labels_list = []\n",
    "        for j in range(batch_size):\n",
    "            partial_json = audio_dataset_section_json[(i*batch_size)+j]\n",
    "            ir_record_json = create_ir_json(partial_json, audio_clip_directory, max_length)\n",
    "            batch_labels_list.append(ir_record_json['category_id'])\n",
    "            batch_features_list.append(ir_record_json['audio_data'])\n",
    "            batch_ir_list.append(ir_record_json.copy())\n",
    "        batch_features_tensor = tf.stack(batch_features_list, axis=0)\n",
    "        yield (batch_ir_list, batch_features_tensor, batch_labels_list)\n",
    "    if(left_over):\n",
    "        partial_json_list = audio_dataset_section_json[-1:-left_over]\n",
    "        batch_ir_list = []\n",
    "        batch_features_list = []\n",
    "        batch_labels_list = []\n",
    "        for idx, partial_json in enumerate(partial_json_list):\n",
    "            ir_record_json = create_ir_json(partial_json, audio_clip_directory, max_length)\n",
    "            batch_labels_list.append(ir_record_json['category_id'])\n",
    "            batch_features_list.append(ir_record_json['audio_data'])\n",
    "            batch_ir_list.append(ir_record_json.copy())\n",
    "        batch_features_tensor = tf.stack(batch_features_list, axis=0)\n",
    "        yield (batch_ir_list, batch_features_tensor, batch_labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct the dataset\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS_COUNT = 256\n",
    "\n",
    "internal_representation_list = convert_classification_audioengine_dataset_to_IR_generator(dataset_json, AUDIO_CLIPS_FULL_DIR_PATH, batch_size=BATCH_SIZE)\n",
    "\n",
    "#Get more information about the dataset\n",
    "num_classes = len(dataset_json['categories'])\n",
    "max_length = get_max_samples_in_wav_from_directory(AUDIO_CLIPS_FULL_DIR_PATH)\n",
    "input_dimension = (BATCH_SIZE, max_length, 1)\n",
    "batch_input_dimension = (BATCH_SIZE, max_length, 1)\n",
    "    \n",
    "#setup model\n",
    "model = Simple1DConvNet(num_classes=num_classes, \n",
    "                        input_dimension=input_dimension, \n",
    "                        batch_input_shape=batch_input_dimension)\n",
    "\n",
    "# setup loss and optimizer\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "\n",
    "def train_model(model, dataset_generator, num_epochs: int = 0, log_step_count: int = 200):        \n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "        # Iterate over the batches of the dataset.\n",
    "        for step, generator_return in enumerate(dataset_generator):            \n",
    "            batch_ir_json, features_tensor, label_list = generator_return\n",
    "            # Open a GradientTape to record the operations run\n",
    "            # during the forward pass, which enables auto-differentiation.\n",
    "            with tf.GradientTape() as tape:\n",
    "\n",
    "                # Run the forward pass of the layer.\n",
    "                # The operations that the layer applies\n",
    "                # to its inputs are going to be recorded\n",
    "                # on the GradientTape.\n",
    "                logits = model(features_tensor, training=True)  # Logits for this minibatch\n",
    "\n",
    "                # Compute the loss value for this minibatch.\n",
    "                loss_value = loss_fn(label_list, logits)\n",
    "\n",
    "            # Use the gradient tape to automatically retrieve\n",
    "            # the gradients of the trainable variables with respect to the loss.\n",
    "            grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "\n",
    "            # Run one step of gradient descent by updating\n",
    "            # the value of the variables to minimize the loss.\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "            # Log every 200 batches.\n",
    "            if step % 200 == 0:\n",
    "                print('Training loss (for one batch) at step {}: {:.4f}'.format(step, float(loss_value)))\n",
    "                print('Seen so far: {} samples'.format((step + 1) * BATCH_SIZE))\n",
    "\n",
    "train_model(model, internal_representation_list, num_epochs=EPOCHS_COUNT, log_step_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
