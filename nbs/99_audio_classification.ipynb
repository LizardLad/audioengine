{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp audio_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import audioengine\n",
    "from audioengine.utils.gpu import list_all_gpus, set_gpu_list_memory_limit\n",
    "from audioengine.utils.schema import verify_audioengine_dataset, verify_audioengine_internal_audio_representation_schema\n",
    "from audioengine.utils.misc import log_init, log_info, log_debug, log_error, pad_up_to\n",
    "from audioengine.models import Simple1DConvNet\n",
    "from audioengine.utils.wav_utils import get_max_samples_in_wav_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DEBUG == True):\n",
    "    #physical_gpus_list = list_all_gpus()\n",
    "    #set_gpu_list_memory_limit(physical_gpus_list, limit=(2**13+2**11)) #8192\n",
    "    \n",
    "    log_init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset\n",
    "\n",
    "This notebook will be using the single word dataset which is a subset of the common voice dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIRECTORY = '/project/Datasets/audioengine_single_word'\n",
    "DATASET_NAME = 'dev.json'\n",
    "AUDIO_CLIPS_DIR_NAME = 'clips'\n",
    "\n",
    "DATASET_JSON_FILEPATH = os.path.join(DATASET_DIRECTORY, DATASET_NAME)\n",
    "AUDIO_CLIPS_FULL_DIR_PATH = os.path.join(DATASET_DIRECTORY, AUDIO_CLIPS_DIR_NAME)\n",
    "\n",
    "dataset_json_fp = open(DATASET_JSON_FILEPATH, 'r')\n",
    "dataset_json = json.load(dataset_json_fp)\n",
    "dataset_json_fp.close()\n",
    "\n",
    "if(not verify_audioengine_dataset(dataset_json)):\n",
    "    log_critical('The dataset does not match the schema!')\n",
    "else:\n",
    "    log_debug('The dataset matches the schema')\n",
    "    \n",
    "def create_ir_json(partial_json: dict, audio_clip_directory: str, length_to_pad_to: int) -> dict:\n",
    "    audio_data_id = partial_json['id']\n",
    "    file_name = partial_json['file_name']\n",
    "    full_audio_clip_filepath = os.path.join(audio_clip_directory, file_name)\n",
    "    contents = tf.io.read_file(full_audio_clip_filepath)\n",
    "    audio_data, _ = tf.audio.decode_wav(contents)\n",
    "    audio_data = tf.squeeze(audio_data, axis=1)\n",
    "    audio_data = pad_up_to(audio_data, (length_to_pad_to,), 0)\n",
    "    audio_data = tf.expand_dims(audio_data, axis=-1)\n",
    "\n",
    "\n",
    "    ir_record_json = {'audio_data': audio_data,\n",
    "                      'length': tf.shape(audio_data)[0],\n",
    "                      'id': audio_data_id,\n",
    "                      'file_name': file_name,\n",
    "                      'category_id': partial_json['category_id']\n",
    "                     }\n",
    "    return ir_record_json\n",
    "\n",
    "def convert_labels_list_to_tensor(label_list):\n",
    "    #new_label_list = []\n",
    "    #for label in label_list:\n",
    "    #    new_label = []\n",
    "    #    for i in range(len(label_list)):\n",
    "    #        new_label.append(0)\n",
    "    #    new_label[label] = 1\n",
    "    #    new_label_list.append(new_label.copy())\n",
    "    #label_tensor = tf.convert_to_tensor(new_label_list)\n",
    "    #return label_tensor\n",
    "    label_tensor = tf.cast(tf.convert_to_tensor(label_list), tf.float32)\n",
    "    one_tensor = tf.constant(1, dtype=tf.float32)\n",
    "    return tf.cast(label_tensor - one_tensor, tf.int32)\n",
    "\n",
    "def convert_classification_audioengine_dataset_to_IR_generator(dataset_json: dict = {}, audio_clip_directory: str = '',\n",
    "                                                               batch_size: int = 256) -> list:\n",
    "    '''\n",
    "    This uses too much memory to hold the whole dataset at once\n",
    "    Need to use generators instead.\n",
    "    '''\n",
    "    #Really needs multiprocessing in the future\n",
    "    if(not dataset_json['info']['task'] == 'classification'):\n",
    "        log_critical('Dataset not using classification task')\n",
    "    else:\n",
    "        log_debug('Dataset using classification task')\n",
    "    \n",
    "    audio_dataset_section_json = dataset_json['audio']\n",
    "    \n",
    "    #Batch it here\n",
    "    num_batches = math.floor((len(audio_dataset_section_json) - (len(audio_dataset_section_json) % batch_size)) / batch_size)\n",
    "    left_over = len(audio_dataset_section_json) % batch_size\n",
    "    \n",
    "    #ir_list = []\n",
    "    max_length = get_max_samples_in_wav_from_directory(audio_clip_directory)\n",
    "    for i in range(num_batches):\n",
    "        batch_ir_list = []\n",
    "        batch_features_list = []\n",
    "        batch_labels_list = []\n",
    "        for j in range(batch_size):\n",
    "            partial_json = audio_dataset_section_json[(i*batch_size)+j]\n",
    "            ir_record_json = create_ir_json(partial_json, audio_clip_directory, max_length)\n",
    "            batch_labels_list.append(ir_record_json['category_id'])\n",
    "            batch_features_list.append(ir_record_json['audio_data'])\n",
    "            batch_ir_list.append(ir_record_json.copy())\n",
    "        batch_features_tensor = tf.cast(tf.stack(batch_features_list, axis=0), tf.float32)\n",
    "        batch_labels_tensor = tf.cast(convert_labels_list_to_tensor(batch_labels_list), tf.int32)\n",
    "        #yield (batch_ir_list, batch_features_tensor, batch_labels_tensor)\n",
    "        yield (batch_features_tensor, batch_labels_tensor)\n",
    "    #if(left_over):\n",
    "    #    partial_json_list = audio_dataset_section_json[-1:-left_over]\n",
    "    #    batch_ir_list = []\n",
    "    #    batch_features_list = []\n",
    "    #    batch_labels_list = []\n",
    "    #    for idx, partial_json in enumerate(partial_json_list):\n",
    "    #        ir_record_json = create_ir_json(partial_json, audio_clip_directory, max_length)\n",
    "    #        batch_labels_list.append(ir_record_json['category_id'])\n",
    "    #        batch_features_list.append(ir_record_json['audio_data'])\n",
    "    #        batch_ir_list.append(ir_record_json.copy())\n",
    "    #    batch_features_tensor = tf.cast(tf.stack(batch_features_list, axis=0), tf.float32)\n",
    "    #    batch_labels_tensor = tf.cast(convert_labels_list_to_tensor(batch_labels_list), tf.int32)\n",
    "    #    #yield (batch_ir_list, batch_features_tensor, batch_labels_tensor)\n",
    "    #    yield (batch_features_tensor, batch_labels_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct the dataset\n",
    "BATCH_SIZE = 10\n",
    "EPOCHS_COUNT = 256\n",
    "\n",
    "#Get more information about the dataset\n",
    "num_classes = len(dataset_json['categories'])\n",
    "max_length = get_max_samples_in_wav_from_directory(AUDIO_CLIPS_FULL_DIR_PATH)\n",
    "input_dimension = (BATCH_SIZE, max_length, 1)\n",
    "batch_input_dimension = (BATCH_SIZE, max_length, 1)\n",
    "    \n",
    "#setup model\n",
    "model = Simple1DConvNet(num_classes=num_classes, \n",
    "                        input_dimension=input_dimension, \n",
    "                        batch_input_shape=batch_input_dimension)\n",
    "\n",
    "# setup loss and optimizer\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1.0,\n",
    "                                                          decay_steps=10000,\n",
    "                                                          decay_rate=0.9)\n",
    "optimizer = tf.keras.optimizers.Adadelta(learning_rate=lr_schedule)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "def train_model_2(model, dataset_generator, num_epochs: int = 0, \n",
    "                  log_step_count: int = 200, dataset_json: dict = {},\n",
    "                  audio_clip_directory: str = '', batch_size: int = 8,\n",
    "                  batch_input_dimension: tuple = (),\n",
    "                  label_tensor_shape: tuple = ()):\n",
    "    generator_partial = functools.partial(dataset_generator, dataset_json=dataset_json, \n",
    "                                          audio_clip_directory=audio_clip_directory, batch_size=batch_size)\n",
    "    dataset = tf.data.Dataset.from_generator(generator_partial, output_signature=(tf.TensorSpec(shape=batch_input_dimension, dtype=tf.float32),\n",
    "                                                                                  tf.TensorSpec(shape=label_tensor_shape, dtype=tf.int32)))\n",
    "    model.fit(x=dataset, epochs=num_epochs)\n",
    "    \n",
    "    \n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "train_model_2(model, convert_classification_audioengine_dataset_to_IR_generator,\n",
    "              num_epochs=EPOCHS_COUNT, log_step_count=1, \n",
    "              dataset_json=dataset_json, audio_clip_directory=AUDIO_CLIPS_FULL_DIR_PATH,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              batch_input_dimension=batch_input_dimension,\n",
    "              label_tensor_shape=(BATCH_SIZE,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
